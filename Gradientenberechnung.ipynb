{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjlhZUAnyIGu1OkQ756g7N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karkessler/llm/blob/main/Gradientenberechnung.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-m_VQHwAk1l"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Gradientenberechnung – Demo für lineare Modelle\n",
        "-----------------------------------------------\n",
        "Passt zum Folienteil:\n",
        "    z = W · x + b\n",
        "    p = z (lineares Modell)\n",
        "    L = (p - y)^2\n",
        "    ∂L/∂W = 2 · (p - y) · x\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Trainingsdaten (1 Feature, 1 Beispiel)\n",
        "x = np.array([2.0])      # Eingabe\n",
        "y = np.array([4.0])      # Zielwert\n",
        "\n",
        "# Initiales Gewicht und Bias\n",
        "W = np.array([0.5])\n",
        "b = 0.0\n",
        "\n",
        "# Lernrate\n",
        "eta = 0.1\n",
        "\n",
        "# Verlauf für Plot\n",
        "weight_history = []\n",
        "loss_history = []\n",
        "\n",
        "def forward(x, W, b):\n",
        "    \"\"\"Lineares Modell: z = W * x + b\"\"\"\n",
        "    return W * x + b\n",
        "\n",
        "def loss(p, y):\n",
        "    \"\"\"Quadratischer Fehler\"\"\"\n",
        "    return (p - y) ** 2\n",
        "\n",
        "def gradient(x, y, W, b):\n",
        "    \"\"\"\n",
        "    ∂L/∂W = 2·(p - y)·x\n",
        "    ∂L/∂b = 2·(p - y)\n",
        "    \"\"\"\n",
        "    p = forward(x, W, b)\n",
        "    dL_dW = 2 * (p - y) * x\n",
        "    dL_db = 2 * (p - y)\n",
        "    return dL_dW, dL_db\n",
        "\n",
        "# Trainingsschritte\n",
        "for step in range(25):\n",
        "    p = forward(x, W, b)\n",
        "    L = loss(p, y)\n",
        "\n",
        "    weight_history.append(W.copy())\n",
        "    loss_history.append(L.item())\n",
        "\n",
        "    dW, db = gradient(x, y, W, b)\n",
        "\n",
        "    # Gradient Descent\n",
        "    W = W - eta * dW\n",
        "    b = b - eta * db\n",
        "\n",
        "# ---- Plot ----\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(weight_history, label=\"W-Verlauf\")\n",
        "plt.xlabel(\"Schritt\")\n",
        "plt.ylabel(\"Gewicht W\")\n",
        "plt.title(\"Gradient Descent auf W\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(loss_history, label=\"Loss\", color=\"red\")\n",
        "plt.xlabel(\"Schritt\")\n",
        "plt.ylabel(\"Loss L\")\n",
        "plt.title(\"Fehlerverlauf\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Finales Gewicht W:\", W)\n",
        "print(\"Finaler Bias b:\", b)\n"
      ]
    }
  ]
}