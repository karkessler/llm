
"""
Tokenisierung des Beispielsatzes aus der Vorlesung:

„Paris ist die Hauptstadt von Frankreich“

Dieses Skript zeigt eine sehr einfache Tokenisierung und eine manuell
definierte Subword-Zerlegung, wie sie in der Vorlesung diskutiert wird.
"""

def simple_whitespace_tokenize(text: str):
    """
    Sehr einfache Tokenisierung: nur nach Leerzeichen trennen.
    (Das ist NICHT das, was ein echtes LLM macht – aber illustriert die Grundidee.)
    """
    return text.split()

def main():
    text = "Paris ist die Hauptstadt von Frankreich"
    print("Originalsatz:", text)
    tokens = simple_whitespace_tokenize(text)
    print("Einfache Tokens:", tokens)

    # Manuelle Subword-Tokens (Beispiel)
    subword_tokens = ["Paris", "Ġis", "t", "Ġdie", "ĠHau", "pt", "stadt", "Ġvon", "ĠFrank", "reich"]
    print("\nBeispiel für Subword-Tokens (angelehnt an BPE-Konzepte):")
    print(subword_tokens)
    print("\nHinweis: In echten Modellen wird die Zerlegung automatisch gelernt (BPE/SentencePiece).")

if __name__ == "__main__":
    main()
