
# Vom Token zur Antwort – LLM-Vorlesung (Online-Version)

Willkommen zur Online-Version der Vorlesung

> „Vom Token zur Antwort – Wie Large Language Models wirklich funktionieren“

## Inhalte

- [Skript als Markdown](../lectures/llm_vorlesung.md)
- Python-Demos:
  - `python -m demos.tokenization_demo`
  - `python -m demos.self_attention_demo`
  - `python -m demos.softmax_demo`
  - `python -m demos.crossentropy_demo`
  - `python -m demos.gradient_descent_demo`

Die Demos illustrieren die mathematischen Rechenschritte aus der Vorlesung:

- Tokenisierung und Subword-Zerlegung
- Self-Attention (Q, K, V, Attention-Matrix)
- Logits und Softmax
- Cross-Entropy-Loss
- Gradientenabstieg
