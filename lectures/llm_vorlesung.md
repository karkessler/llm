
# LLM-Vorlesung – Überblick

Dieses Skript beschreibt in Textform die Inhalte der Vorlesung
„Vom Token zur Antwort – Wie Large Language Models wirklich funktionieren“.
Es ergänzt das Folienskript mit Bildern (`llm_vorlesung_slides.md`) und die
Python-Demos im Ordner `demos/`.

Die Kapitel:
1. Motivation
2. Transformer-Architektur & Self-Attention
3. Mathematische Grundlagen
4. Rechenbeispiel: Self-Attention für das Token „von“
5. Lernprozess & Backpropagation
6. Decoder-Vorhersage
7. Modellarchitekturen im Überblick
8. Titans – Ausblick auf neue Architekturen
9. Grenzen klassischer LLMs
10. Fazit & Diskussion
11. Glossar

Die Details sind bewusst knapp gehalten, weil die eigentliche Lehre über
die Folien + Demos erfolgt.
