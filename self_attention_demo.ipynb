{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPB84fjTPmob/9mI9dIBk+D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karkessler/llm/blob/main/self_attention_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06q9j20cZzAs"
      },
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"\n",
        "Self-Attention-Minibeispiel für das Token „von“ im Satz\n",
        "\n",
        "„Paris ist die Hauptstadt von Frankreich“\n",
        "\n",
        "Wir definieren:\n",
        "- einfache Embeddings X\n",
        "- Gewichtsmatrizen W_Q, W_K, W_V (hier Einheitsmatrizen)\n",
        "- berechnen Q, K, V\n",
        "- berechnen Attention-Scores und den neuen kontextualisierten Vektor für „von“.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def softmax(x: np.ndarray) -> np.ndarray:\n",
        "    e = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "    return e / e.sum(axis=-1, keepdims=True)\n",
        "\n",
        "def main():\n",
        "    tokens = [\"Paris\", \"ist\", \"die\", \"Hauptstadt\", \"von\"]\n",
        "    n_tokens = len(tokens)\n",
        "    d_model = 4\n",
        "\n",
        "    np.random.seed(42)\n",
        "    X = np.random.rand(n_tokens, d_model)\n",
        "\n",
        "    W_Q = np.eye(d_model)\n",
        "    W_K = np.eye(d_model)\n",
        "    W_V = np.eye(d_model)\n",
        "\n",
        "    Q = X @ W_Q.T\n",
        "    K = X @ W_K.T\n",
        "    V = X @ W_V.T\n",
        "\n",
        "    idx_von = tokens.index(\"von\")\n",
        "    q_von = Q[idx_von:idx_von+1, :]\n",
        "\n",
        "    scores = q_von @ K.T\n",
        "    weights = softmax(scores)\n",
        "    context_von = weights @ V\n",
        "\n",
        "    print(\"Tokens:\", tokens)\n",
        "    print(\"\\nEmbeddings X:\")\n",
        "    print(X)\n",
        "    print(\"\\nQuery von 'von':\")\n",
        "    print(q_von)\n",
        "    print(\"\\nAttention-Scores (vor Softmax) für 'von' auf alle Tokens:\")\n",
        "    print(scores)\n",
        "    print(\"\\nAttention-Gewichte (nach Softmax) für 'von':\")\n",
        "    for t, w in zip(tokens, weights.flatten()):\n",
        "        print(f\"  Attention auf {t:10s}: {w:.3f}\")\n",
        "    print(\"\\nNeuer kontextualisierter Vektor für 'von':\")\n",
        "    print(context_von)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}